花了太多时间在前面深究代码上了，后面很多概念弄得有点一知半解。
# LSTM和transformer的具体实现有点头疼，后面有空要回头看一下
# 协变量偏移和标签偏移
一个是用来训练的样本和测试的样本的分布不一样，比如拿照片训练，用卡通图片来检验；

# 错题记录

下列哪一项不是构建Vocab类所必须的步骤：
词频统计，清洗低频词
句子长度统计
构建索引到token的映射
构建token到索引的映射
答案解释
句子长度统计与构建字典不相关
总结句子长度和字典是没有关系的

在LSTM模型的初始化中，下列不需要初始化的参数是？
每个循环单元中的记忆细胞和循环单元的值
第0个循环单元的记忆细胞和循环单元的值
门控单元中用于计算遗忘门的权重与偏差
用于计算输出的权重与偏差
答案解释
每个循环单元中的记忆细胞和循环单元的值为LSTM模型中的隐状态，而非参数，因此不需要初始化。
总结第0个循环单元是需要初始化的

在通过卷积层或池化层后，输出的高和宽可能减小，为了尽可能保留输入的特征，我们可以在减小高宽的同时增加通道数
选项1：错误，参考视频1分钟左右对全连接层局限性的介绍

选项2：正确，参考LeNet中的池化层

选项3：正确，参考视频1分30左右对卷积层优势的介绍

选项4：正确，参考视频3分钟左右的介绍
关于AlexNet描述错误的是

用Dropout来控制全连接层的模型复杂度

包含有5层卷积和2层全连接隐藏层，以及1个全连接输出层

将LeNet中的ReLU激活函数改成了sigmoid激活函数。

首次证明了学习到的特征可以超越⼿⼯设计的特征
选项1：正确，参考AlexNet的特征。

选项2：正确，参考AlexNet的特征。

选项3：错误，应该是将LeNet中的sigmoid激活函数改成了ReLU激活函数。

选项4：正确，在2012 ImageNet竞赛夺魁。
关于VGG描述正确的是

使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。

通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息。

与AlexNet相比，难以灵活地改变模型结构。

通过重复使⽤简单的基础块来构建深度模型。
选项1：错误，描述的是NiN。

选项2：错误，描述的是GoogLeNet。

选项3：错误，VGG相对AlexNet网络表达能力更强。

选项4：正确，参考VGG结构。
在LSTM模型的初始化中，下列不需要初始化的参数是？

每个循环单元中的记忆细胞和循环单元的值

第0个循环单元的记忆细胞和循环单元的值

门控单元中用于计算遗忘门的权重与偏差

用于计算输出的权重与偏差
每个循环单元中的记忆细胞和循环单元的值为LSTM模型中的隐状态，而非参数，因此不需要初始化。

# 
